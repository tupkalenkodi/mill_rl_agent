Index: dqn/train.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\nfrom famnit_gym.envs import mill\nfrom dqn.agent import DQNAgent\n\n\n# In your training script, add this helper\ndef calculate_reward(old_state, new_state, player_id, capture_made=False):\n    \"\"\"\n    Calculate sophisticated reward using minimax evaluation\n    \"\"\"\n    # Your existing evaluate_state function (adapted)\n    def evaluate_state(state_model, player):\n        opponent = 3 - player\n        p1_pieces = state_model.count_pieces(player)\n        p2_pieces = state_model.count_pieces(opponent)\n        piece_advantage = (p1_pieces - p2_pieces) * 30\n\n        # Position evaluation\n        position_advantage = 0\n        board_state = state_model.get_state()\n\n        position_values = {\n            4: 4, 5: 4, 6: 4, 14: 4, 21: 4, 20: 4, 19: 4, 11: 4,\n            1: 3, 2: 3, 3: 3, 15: 3, 24: 3, 23: 3, 22: 3, 10: 3,\n            7: 3, 8: 3, 9: 3, 13: 3, 18: 3, 17: 3, 16: 3, 12: 3\n        }\n\n        for pos, value in position_values.items():\n            if board_state[pos - 1] == player:\n                position_advantage += value\n            elif board_state[pos - 1] == opponent:\n                position_advantage -= value\n\n        total_score = piece_advantage + position_advantage\n        normalized_score = total_score / 240.0  # Normalize to [-1, 1]\n        return normalized_score\n\n    # Calculate scores\n    old_score = evaluate_state(old_state, player_id)\n    new_score = evaluate_state(new_state, player_id)\n\n    # Improvement\n    total_reward = new_score - old_score\n\n    return total_reward\n\n\ndef train_dqn(num_episodes=1000, target_update_freq=10, save_freq=100):\n    \"\"\"Train DQN agent through self-play\"\"\"\n\n    # Create environment\n    env = mill.env(render_mode=None)\n\n    # Create two agents (self-play)\n    agent1 = DQNAgent(player_id=1)\n    agent2 = DQNAgent(player_id=2)\n\n    episode_rewards = []\n\n    for episode in range(num_episodes):\n        env.reset()\n\n        episode_reward_1 = 0\n        episode_reward_2 = 0\n        step_count = 0\n\n        for agent_name in env.agent_iter():\n            observation, env_reward, termination, truncation, info = env.last()\n\n            # Get current player and state\n            current_player = 1 if agent_name == \"player_1\" else 2\n            state = observation  # Board state\n\n            # Choose agent\n            agent = agent1 if current_player == 1 else agent2\n\n            # Episode done\n            if termination or truncation:\n                # Store final transition with terminal reward\n                if current_player == 1:\n                    episode_reward_1 += env_reward\n                    agent1.store_transition(state, [0, 0, 0], env_reward, state, True)\n                else:\n                    episode_reward_2 += env_reward\n                    agent2.store_transition(state, [0, 0, 0], env_reward, state, True)\n                break\n\n\n            # Get legal moves and choose action\n            legal_moves = info['legal_moves']\n            action = agent.choose_move(state, legal_moves)\n\n            # Store state BEFORE the move\n            state_before_move = mill.transition_model(env)\n\n            # Take step\n            env.step(action)\n\n            # Get state AFTER the move\n            state_after_move = mill.transition_model(env)\n\n            # Get next state and reward\n            next_observation, _, _, _, _ = env.last()\n\n            next_reward = calculate_reward(\n                state_before_move,\n                state_after_move,\n                current_player\n            )\n\n            # Store transition\n            if current_player == 1:\n                episode_reward_1 += next_reward\n                agent1.store_transition(state, action, next_reward, next_observation, False)\n            else:\n                episode_reward_2 += next_reward\n                agent2.store_transition(state, action, next_reward, next_observation, False)\n\n            # Train both agents\n            loss1 = agent1.train()\n            loss2 = agent2.train()\n\n            step_count += 1\n\n        # Decay epsilon\n        agent1.decay_epsilon()\n        agent2.decay_epsilon()\n\n        # Update target networks\n        if episode % target_update_freq == 0:\n            agent1.update_target_network()\n            agent2.update_target_network()\n\n        # Save models\n        if episode % save_freq == 0 and episode > 0:\n            agent1.save(f'dqn_models/agent1_episode_{episode}.pth')\n            agent2.save(f'dqn_models/agent2_episode_{episode}.pth')\n\n        # Logging\n        episode_rewards.append((episode_reward_1, episode_reward_2))\n\n        if episode % 10 == 0:\n            avg_reward_1 = np.mean([r[0] for r in episode_rewards[-10:]])\n            avg_reward_2 = np.mean([r[1] for r in episode_rewards[-10:]])\n            print(f\"Episode {episode}/{num_episodes}\")\n            print(f\"  Avg Reward P1: {avg_reward_1:.2f}, P2: {avg_reward_2:.2f}\")\n            print(f\"  Epsilon: {agent1.epsilon:.3f}\")\n            print(f\"  Steps: {step_count}\")\n\n    # Final save\n    agent1.save('dqn_models/agent1_final.pth')\n    agent2.save('dqn_models/agent2_final.pth')\n\n    print(\"Training complete!\")\n\n    return agent1, agent2, episode_rewards\n\n\nif __name__ == \"__main__\":\n    import os\n\n    os.makedirs('dqn_models', exist_ok=True)\n\n    train_dqn(num_episodes=100, target_update_freq=10, save_freq=100)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dqn/train.py b/dqn/train.py
--- a/dqn/train.py	(revision 7beb1d92f8c95116c329042cd362146ba31b69d2)
+++ b/dqn/train.py	(date 1767432952498)
@@ -3,8 +3,43 @@
 from dqn.agent import DQNAgent
 
 
+def preprocess_state(env, player_name):
+    """Create enhanced state representation"""
+    # Get board state
+    board = env.observe(player_name)  # Shape (24,)
+
+    # Get player info
+    player_idx = 0 if player_name == "player_1" else 1
+
+    # Get phase info from model
+    player_id = player_idx + 1  # Convert to 1 or 2
+    phase = env._model._player[player_id]['phase']
+
+    # One-hot encode phase
+    phase_mapping = {'placing': [1, 0, 0], 'moving': [0, 1, 0], 'flying': [0, 0, 1]}
+    phase_encoded = phase_mapping.get(phase, [0, 0, 0])
+
+    # Pieces info
+    pieces_holding = env._model._player[player_id]['pieces_holding']
+    pieces_playing = env._model._player[player_id]['pieces_playing']
+
+    # Opponent pieces
+    opponent_id = 3 - player_id
+    opp_pieces = env._model.count_pieces(opponent_id)
+
+    # Concatenate all features
+    state = np.concatenate([
+        board,
+        [player_idx, 1 - player_idx],  # One-hot player
+        phase_encoded,
+        [pieces_holding / 9.0, pieces_playing / 9.0],  # Normalized
+        [opp_pieces / 9.0]
+    ])
+
+    return state.astype(np.float32)
+
 # In your training script, add this helper
-def calculate_reward(old_state, new_state, player_id, capture_made=False):
+def calculate_reward(old_state, new_state, player_id):
     """
     Calculate sophisticated reward using minimax evaluation
     """
@@ -41,10 +76,180 @@
 
     # Improvement
     total_reward = new_score - old_score
+    print(total_reward)
 
     return total_reward
 
 
+def calculate_reward2(env, old_state_model, new_state_model, player_id, action):
+    """
+    Calculate reward with multiple components
+    """
+    opponent_id = 3 - player_id
+
+    # 1. Win/loss reward (most important)
+    if new_state_model.game_over():
+        winner = 1 if new_state_model._player[2]['phase'] == 'lost' else 2
+        return 10.0 if player_id == winner else -10.0
+
+    # 2. Capture reward
+    reward = 0
+    pieces_before = old_state_model.count_pieces(opponent_id)
+    pieces_after = new_state_model.count_pieces(opponent_id)
+    if pieces_after < pieces_before:
+        reward += 2.0  # Reward for capturing
+
+    # 3. Mill formation reward
+    src, dst, capture = action
+    if dst > 0:  # Check if destination forms a mill
+        # Check all mills containing dst
+        for mill in new_state_model.mills:
+            if dst in mill:
+                # Check if all positions in mill are player's pieces
+                if all(new_state_model._board[pos] == player_id for pos in mill):
+                    reward += 1.5  # Bonus for forming a mill
+
+    # 4. Strategic positioning
+    # Reward controlling center and intersections
+    strategic_positions = [5, 8, 11, 14, 17, 20]
+    if dst in strategic_positions and new_state_model._board[dst] == player_id:
+        reward += 0.3
+
+    # 5. Phase advancement reward
+    old_phase = old_state_model._player[player_id]['phase']
+    new_phase = new_state_model._player[player_id]['phase']
+    if new_phase == 'flying' and old_phase != 'flying':
+        reward += 0.5  # Bonus for reaching flying phase
+
+    # 6. Small penalty for moving (encourage decisive play)
+    reward -= 0.01
+
+    return float(reward)
+
+
+def evaluate_agents(agent1, agent2, env, num_eval_games=10):
+    """Evaluate agents against each other"""
+    p1_wins = 0
+    p2_wins = 0
+    draws = 0
+
+    for game in range(num_eval_games):
+        env.reset()
+
+        for agent_name in env.agent_iter():
+            observation, _, termination, truncation, info = env.last()
+
+            if termination or truncation:
+                if termination:
+                    winner = 1 if env._model._player[2]['phase'] == 'lost' else 2
+                    if winner == 1:
+                        p1_wins += 1
+                    else:
+                        p2_wins += 1
+                else:
+                    draws += 1
+                break
+
+            # Choose action (disable exploration)
+            legal_moves = info['legal_moves']
+            if len(legal_moves) == 0:
+                env.step([0, 0, 0])
+                continue
+
+            # Use greedy policy for evaluation
+            agent = agent1 if agent_name == "player_1" else agent2
+            old_epsilon = agent.epsilon
+            agent.epsilon = 0.01  # Minimal exploration
+
+            action = agent.choose_move(observation, legal_moves)
+            agent.epsilon = old_epsilon
+
+            env.step(action)
+
+    return {
+        'p1_wins': p1_wins / num_eval_games,
+        'p2_wins': p2_wins / num_eval_games,
+        'draws': draws / num_eval_games
+    }
+
+
+def train_dqn2(num_episodes=1000):
+    env = mill.env(render_mode=None)
+
+    agent1 = DQNAgent(player_id=1)
+    agent2 = DQNAgent(player_id=2)
+
+    for episode in range(num_episodes):
+        env.reset()
+
+        for agent_name in env.agent_iter():
+            observation, env_reward, termination, truncation, info = env.last()
+
+            if termination or truncation:
+                # Get final state
+                state = preprocess_state(env, agent_name)
+
+                # Determine winner
+                if env._model.game_over():
+                    winner = 1 if env._model._player[2]['phase'] == 'lost' else 2
+                    current_player_id = 1 if agent_name == "player_1" else 2
+                    final_reward = 10.0 if current_player_id == winner else -10.0
+                else:
+                    final_reward = 0.0  # Draw
+
+                # Store final transition
+                agent = agent1 if agent_name == "player_1" else agent2
+                # Use a dummy action that won't cause index issues
+                agent.store_transition(state, [0, 0, 0], final_reward, state, True)
+                break
+
+            # Get current state
+            state_before = preprocess_state(env, agent_name)
+            current_player_id = 1 if agent_name == "player_1" else 2
+
+            # Store model before move
+            model_before = mill.transition_model(env)
+
+            # Choose action
+            legal_moves = info['legal_moves']
+            if len(legal_moves) == 0:
+                # No legal moves - game over
+                env.step([0, 0, 0])
+                continue
+
+            agent = agent1 if agent_name == "player_1" else agent2
+            action = agent.choose_move(state_before[:24], legal_moves)  # Use only board part
+
+            # Take step
+            env.step(action)
+
+            # Get state after move
+            state_after = preprocess_state(env, agent_name)
+            model_after = mill.transition_model(env)
+
+            # Calculate reward
+            reward = calculate_reward2(env, model_before, model_after, current_player_id, action)
+
+            # Store transition
+            agent.store_transition(state_before, action, reward, state_after, False)
+
+            # Train agent
+            loss = agent.train()
+
+        # Decay epsilon and update target
+        agent1.decay_epsilon()
+        agent2.decay_epsilon()
+
+        if episode % 10 == 0:
+            agent1.update_target_network()
+            agent2.update_target_network()
+
+            # Evaluate agents
+            eval_result = evaluate_agents(agent1, agent2, env, num_eval_games=10)
+            print(f"Episode {episode}: Win rates - P1: {eval_result['p1_wins']:.2f}, "
+                  f"P2: {eval_result['p2_wins']:.2f}, Epsilon: {agent1.epsilon:.3f}")
+
+
 def train_dqn(num_episodes=1000, target_update_freq=10, save_freq=100):
     """Train DQN agent through self-play"""
 
@@ -56,12 +261,17 @@
     agent2 = DQNAgent(player_id=2)
 
     episode_rewards = []
+    episode_losses = []
+    episode_step_counts = []
 
-    for episode in range(num_episodes):
+    for episode in range(1, num_episodes+1):
         env.reset()
 
         episode_reward_1 = 0
         episode_reward_2 = 0
+        episode_loss_1 = 0
+        episode_loss_2 = 0
+
         step_count = 0
 
         for agent_name in env.agent_iter():
@@ -116,9 +326,15 @@
                 episode_reward_2 += next_reward
                 agent2.store_transition(state, action, next_reward, next_observation, False)
 
-            # Train both agents
-            loss1 = agent1.train()
-            loss2 = agent2.train()
+            # Train only the agent who just acted
+            if current_player == 1:
+                loss = agent1.train()
+                if loss is not None:  # Handle case where buffer isn't full yet
+                    episode_loss_1 += loss
+            else:
+                loss = agent2.train()
+                if loss is not None:
+                    episode_loss_2 += loss
 
             step_count += 1
 
@@ -138,14 +354,19 @@
 
         # Logging
         episode_rewards.append((episode_reward_1, episode_reward_2))
-
+        episode_losses.append((episode_loss_1, episode_loss_2))
+        episode_step_counts.append(step_count)
         if episode % 10 == 0:
             avg_reward_1 = np.mean([r[0] for r in episode_rewards[-10:]])
             avg_reward_2 = np.mean([r[1] for r in episode_rewards[-10:]])
+            avg_loss_1 = np.mean([r[0] for r in episode_losses[-10:]])
+            avg_loss_2 = np.mean([r[1] for r in episode_losses[-10:]])
+            avg_steps = np.mean(episode_step_counts)
             print(f"Episode {episode}/{num_episodes}")
             print(f"  Avg Reward P1: {avg_reward_1:.2f}, P2: {avg_reward_2:.2f}")
+            print(f"  Avg Loss P1: {avg_loss_1:.2f}, P2: {avg_loss_2:.2f}")
             print(f"  Epsilon: {agent1.epsilon:.3f}")
-            print(f"  Steps: {step_count}")
+            print(f"  Steps: {avg_steps}")
 
     # Final save
     agent1.save('dqn_models/agent1_final.pth')
@@ -161,4 +382,4 @@
 
     os.makedirs('dqn_models', exist_ok=True)
 
-    train_dqn(num_episodes=100, target_update_freq=10, save_freq=100)
+    train_dqn2(num_episodes=1000)
Index: dqn/network.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch\nimport torch.nn as nn\n\n\nclass DQNNetwork(nn.Module):\n    def __init__(self, state_size=24, action_size=25 * 25 * 25):\n        super(DQNNetwork, self).__init__()\n\n        # Simple feedforward network\n        self.fc1 = nn.Linear(state_size, 128)\n        self.fc2 = nn.Linear(128, 256)\n        self.fc3 = nn.Linear(256, 128)\n        self.fc4 = nn.Linear(128, action_size)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = torch.relu(self.fc3(x))\n        return self.fc4(x)  # Q-values for all actions\n    
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dqn/network.py b/dqn/network.py
--- a/dqn/network.py	(revision 7beb1d92f8c95116c329042cd362146ba31b69d2)
+++ b/dqn/network.py	(date 1767432692819)
@@ -3,18 +3,28 @@
 
 
 class DQNNetwork(nn.Module):
-    def __init__(self, state_size=24, action_size=25 * 25 * 25):
+    def __init__(self, input_size=31, action_size=25 * 25 * 25):
         super(DQNNetwork, self).__init__()
 
-        # Simple feedforward network
-        self.fc1 = nn.Linear(state_size, 128)
-        self.fc2 = nn.Linear(128, 256)
-        self.fc3 = nn.Linear(256, 128)
-        self.fc4 = nn.Linear(128, action_size)
+        # Input: 24(board) + 2(player) + 3(phase) + 2(pieces) = 31
+        self.fc1 = nn.Linear(input_size, 256)
+        self.bn1 = nn.BatchNorm1d(256)
+
+        self.fc2 = nn.Linear(256, 512)
+        self.bn2 = nn.BatchNorm1d(512)
+
+        self.fc3 = nn.Linear(512, 256)
+        self.bn3 = nn.BatchNorm1d(256)
+
+        self.fc4 = nn.Linear(256, action_size)
+
+        self.dropout = nn.Dropout(0.3)
 
     def forward(self, x):
-        x = torch.relu(self.fc1(x))
-        x = torch.relu(self.fc2(x))
-        x = torch.relu(self.fc3(x))
-        return self.fc4(x)  # Q-values for all actions
+        x = torch.relu(self.bn1(self.fc1(x)))
+        x = self.dropout(x)
+        x = torch.relu(self.bn2(self.fc2(x)))
+        x = self.dropout(x)
+        x = torch.relu(self.bn3(self.fc3(x)))
+        return self.fc4(x)
     
\ No newline at end of file
Index: requirements.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>cloudpickle==3.1.1\r\ncontourpy==1.3.3\r\ncycler==0.12.1\r\nfamnit-gym @ git+https://github.com/DomenSoberlFamnit/famnit-gym@924f82247386bb33a0899a94042d53ef5f086d0a\r\nFarama-Notifications==0.0.4\r\nfonttools==4.60.1\r\ngymnasium==1.2.1\r\nimageio-ffmpeg==0.6.0\r\nkiwisolver==1.4.9\r\nmatplotlib==3.10.7\r\nnumpy==2.3.4\r\npackaging==25.0\r\npettingzoo==1.24.3\r\npillow==12.0.0\r\nprogress==1.6.1\r\npygame==2.6.1\r\npyparsing==3.2.5\r\npython-dateutil==2.9.0.post0\r\nsix==1.17.0\r\ntabulate==0.9.0\r\ntyping_extensions==4.15.0\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-16LE
===================================================================
diff --git a/requirements.txt b/requirements.txt
--- a/requirements.txt	(revision 7beb1d92f8c95116c329042cd362146ba31b69d2)
+++ b/requirements.txt	(date 1767385731459)
@@ -1,3 +1,4 @@
+torch
 cloudpickle==3.1.1
 contourpy==1.3.3
 cycler==0.12.1
Index: mill_env/model.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mill_env/model.py b/mill_env/model.py
new file mode 100644
--- /dev/null	(date 1767432155903)
+++ b/mill_env/model.py	(date 1767432155903)
@@ -0,0 +1,256 @@
+class MillModel:
+    # Define all possible mill triplets.
+    mills = [
+        [1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12],
+        [13, 14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24],
+        [1, 10, 22], [4, 11, 19], [7, 12, 16], [2, 5, 8],
+        [17, 20, 23], [9, 13, 18], [6, 14, 21], [3, 15, 24],
+        [1, 4, 7], [3, 6, 9], [16, 19, 22], [18, 21, 24]
+    ]
+
+    def __init__(self):
+        self._board = [0 for _ in range(25)]
+
+        # Player with index 0 is a dummy to avoid index shifting.
+        self._player = [
+            {'phase': 'dummy', 'pieces_holding': 0, 'pieces_playing': 0},
+            {'phase': 'placing', 'pieces_holding': 9, 'pieces_playing': 0},
+            {'phase': 'placing', 'pieces_holding': 9, 'pieces_playing': 0}
+        ]
+
+        # Compute the connections from the mills.
+        self.connections = []
+        for [a, b, c] in self.mills:
+            self.connections.extend([[a, b], [b, c]])
+
+    def clone(self):
+        # Create and return a cuplicate of itself.
+        board = MillModel()
+        board._board = list(self._board)
+        board._player = [dict(player) for player in self._player]
+        return board
+
+    def get_state(self):
+        return self._board[1:]
+
+    def get_phase(self, player):
+        return self._player[player]['phase']
+
+    def _in_mill(self, position):
+        # Is the piece at the given position a part of a formed mill?
+        for mill in self.mills:
+            if position in mill:
+                if self._board[mill[0]] == self._board[mill[1]] == self._board[mill[2]]:
+                    return True
+        return False
+
+    def _all_pieces(self, player):
+        # Return all the pieces of the given player.
+        positions = []
+        for (position, piece) in enumerate(self._board):
+            if piece == player:
+                positions.append(position)
+        return positions
+
+    def _free_pieces(self, player):
+        # Return all pieces for of the given player that are not in a mill formation.
+        positions = []
+        for (position, piece) in enumerate(self._board):
+            if piece == player and not self._in_mill(position):
+                positions.append(position)
+        return positions
+
+    def _capture_pieces(self, player):
+        # Return all pieces of the given player that can be captured.
+        positions = self._free_pieces(player)
+        if len(positions) == 0:
+            positions = self._all_pieces(player)
+        return positions
+
+    def count_pieces(self, player):
+        return len(self._all_pieces(player))
+
+    def legal_moves(self, player):
+        # If game has finished, there are no legal moves.
+        if self.game_over():
+            return []
+
+        player_info = self._player[player]
+        opponent = 2 if player == 1 else 1
+        moves = []
+
+        # If the player is in the placing phase.
+        if player_info['phase'] == 'placing':
+            # Check all board positions.
+            for (dst, piece) in enumerate(self._board):
+                # Can only place on any empty position.
+                if dst == 0 or piece != 0:
+                    continue
+
+                # Do the move.
+                self._board[dst] = player
+
+                # If a mill has been formed, an opponent's piece must be captured.
+                if self._in_mill(dst):
+                    for piece in self._capture_pieces(opponent):
+                        moves.append([0, dst, piece])
+
+                # If a mill has not been formed, non-capturing move is possible.
+                else:
+                    moves.append([0, dst, 0])
+
+                # Undo the move.
+                self._board[dst] = 0
+
+        # If the player is in the flying phase.
+        elif player_info['phase'] == 'moving':
+            # Check all board positions.
+            for (src, src_piece) in enumerate(self._board):
+                # Can only move it's own piece.
+                if src == 0 or src_piece != player:
+                    continue
+
+                # Check all connections.Â¸
+                for connection in self.connections:
+                    if src not in connection:
+                        continue
+
+                    # Get the connecting position.
+                    dst = connection[0] if src != connection[0] else connection[1]
+
+                    # If destination is empty, the player can move the piece there.
+                    if self._board[dst] == 0:
+                        # Do the move.
+                        self._board[src] = 0
+                        self._board[dst] = player
+
+                        # If a mill has been formed, an opponent's piece must be captured.
+                        if self._in_mill(dst):
+                            for piece in self._capture_pieces(opponent):
+                                moves.append([src, dst, piece])
+
+                        # If a mill has not been formed, non-capturing move is possible.
+                        else:
+                            moves.append([src, dst, 0])
+
+                        # Undo the move.
+                        self._board[src] = player
+                        self._board[dst] = 0
+
+        # If the player is in the flying phase.
+        elif player_info['phase'] == 'flying':
+            # Check all board positions.
+            for (src, src_piece) in enumerate(self._board):
+                # Can only move it's own piece.
+                if src == 0 or src_piece != player:
+                    continue
+
+                # Check all empty positions.
+                for (dst, dst_piece) in enumerate(self._board):
+                    # Can only move to an empty position.
+                    if dst == 0 or dst_piece != 0:
+                        continue
+
+                    # Do the move.
+                    self._board[src] = 0
+                    self._board[dst] = player
+
+                    # If a mill has been formed, an opponent's piece must be captured.
+                    if self._in_mill(dst):
+                        for piece in self._capture_pieces(opponent):
+                            moves.append([src, dst, piece])
+
+                    # If a mill has not been formed, non-capturing move is possible.
+                    else:
+                        moves.append([src, dst, 0])
+
+                    # Undo the move.
+                    self._board[src] = player
+                    self._board[dst] = 0
+
+        return moves
+
+    def make_move(self, player, move):
+        (src, dst, take) = move
+
+        player_info = self._player[player]
+        opponent = 2 if player == 1 else 1
+        captured = 0
+
+        # If the player is in the placing phase.
+        if player_info['phase'] == 'placing':
+            self._board[dst] = player
+
+            player_info['pieces_playing'] += 1
+            player_info['pieces_holding'] -= 1
+
+            if player_info['pieces_holding'] == 0:
+                player_info['phase'] = 'moving'
+
+        # If the player is in the moving phase.
+        elif player_info['phase'] == 'moving':
+            self._board[src] = 0
+            self._board[dst] = player
+
+        # If the player is in the flying phase.
+        elif player_info['phase'] == 'flying':
+            self._board[src] = 0
+            self._board[dst] = player
+
+        # If in any other phase, ignore the move.
+        else:
+            return {}
+
+        # Get the information about the opponent.
+        opponent_info = self._player[opponent]
+
+        # If a piece is taken, consider what happens with the opponent.
+        if take > 0:
+            opponent_info['pieces_playing'] -= 1
+            self._board[take] = 0
+            captured = 1
+
+            # The opponent goes from moving to flying.
+            if opponent_info['phase'] == 'moving':
+                if opponent_info['pieces_playing'] <= 3:
+                    opponent_info['phase'] = 'flying'
+
+            # The opponent goes from flying to losing.
+            elif opponent_info['phase'] == 'flying':
+                if opponent_info['pieces_playing'] <= 2:
+                    opponent_info['phase'] = 'lost'
+
+        # Check if the opponent can make moves.
+        if len(self.legal_moves(opponent)) == 0:
+            # If not, the opponent lost the game.
+            opponent_info['phase'] = 'lost'
+
+        # Return the info.
+        move_info = {
+            'player_phase': player_info['phase'],
+            'opponent_phase': opponent_info['phase'],
+            'pieces_holding': player_info['pieces_holding'],
+            'pieces_playing': player_info['pieces_playing'],
+            'pieces_captured': captured
+        }
+
+        return move_info
+
+    def game_over(self):
+        return self._player[1]['phase'] == 'lost' or self._player[2]['phase'] == 'lost'
+
+    def __str__(self):
+        b = self._board
+        return (
+            f"{b[1]}---------{b[2]}---------{b[3]}\n"
+            f"| \\       |       / |\n"
+            f"|   {b[4]}-----{b[5]}-----{b[6]}   |\n"
+            f"|   | \\   |   / |   |\n"
+            f"|   |   {b[7]}-{b[8]}-{b[9]}   |   |\n"
+            f"{b[10]}---{b[11]}---{b[12]}   {b[13]}---{b[14]}---{b[15]}\n"
+            f"|   |   {b[16]}-{b[17]}-{b[18]}   |   |\n"
+            f"|   | /   |   \\ |   |\n"
+            f"|   {b[19]}-----{b[20]}-----{b[21]}   |\n"
+            f"| /       |       \\ |\n"
+            f"{b[22]}---------{b[23]}---------{b[24]}"
+        )
\ No newline at end of file
Index: mill_env/env.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mill_env/env.py b/mill_env/env.py
new file mode 100644
--- /dev/null	(date 1767432136577)
+++ b/mill_env/env.py	(date 1767432136577)
@@ -0,0 +1,394 @@
+import functools
+import numpy as np
+import gymnasium as gym
+from pettingzoo import AECEnv
+from pettingzoo.utils import agent_selector as AgentSelector
+
+from famnit_gym.envs.mill.mill_model import MillModel
+
+
+# Create the Mill environment.
+def env(render_mode=None):
+    internal_render_mode = None if render_mode != "human" else render_mode
+    env = MillEnv(render_mode=render_mode)
+    return env
+
+
+# Return the Mill transition model for off-line computations.
+def transition_model(env):
+    if type(env) is not MillEnv:
+        raise AttributeError(f'The environment must be an instance of the MillEnv class.')
+    return env._model.clone()
+
+
+class MillEnv(AECEnv):
+    metadata = {
+        "framework": "PettingZoo",
+        "name": "rps_v2",
+        "render_modes": ["ansi", "human"],
+        "render_fps": 60
+    }
+
+    def __init__(self, render_mode=None):
+        self.render_mode = render_mode
+
+        # The names of the players.
+        self.possible_agents = ["player_1", "player_2"]
+
+        # Translation from "player_<idx>" to integer idx.
+        self.agent_index = dict(
+            zip(self.possible_agents, [i + 1 for i in range(len(self.possible_agents))])
+        )
+
+        # Actions are [from, to, capture]; 0 means ignore, 1 - 24 are board positions.
+        self._action_space = gym.spaces.MultiDiscrete(np.array([25, 25, 25]))
+
+        # Observation is an array of board positions: 0 - empty, 1 - player_1, 2 - player_2.
+        self._observation_space = gym.spaces.Box(
+            low=0, high=2,
+            shape=(24,),
+            dtype=np.uint8
+        )
+
+        # The model gets created at reset.
+        self._model = None
+
+        # Do we use pygame?
+        self._pygame_initialized = False
+
+        # If render mode is human, initialize pygame.
+        if render_mode == "human":
+            global pygame
+            import pygame
+            import pygame.gfxdraw
+
+            pygame.init()
+            self._surface = pygame.display.set_mode((704, 704))
+            pygame.display.set_caption("Mill")
+            self._clock = pygame.time.Clock()
+            self._pygame_initialized = True
+
+            self._render_positions = [
+                (0, 0), (0, 3), (0, 6), (1, 1), (1, 3), (1, 5),
+                (2, 2), (2, 3), (2, 4), (3, 0), (3, 1), (3, 2),
+                (3, 4), (3, 5), (3, 6), (4, 2), (4, 3), (4, 4),
+                (5, 1), (5, 3), (5, 5), (6, 0), (6, 3), (6, 6)
+            ]
+
+            self._animation = None
+
+            # Wrappers can set a frame callback that is called before updating the frame.
+            self._frame_callback = None
+
+    @functools.lru_cache(maxsize=None)
+    def observation_space(self, agent):
+        return self._observation_space
+
+    @functools.lru_cache(maxsize=None)
+    def action_space(self, agent):
+        return self._action_space
+
+    def observe(self, agent):
+        # All agents observe the same board.
+        return np.array(self._model._board)[1:]
+
+    def _get_opponent(self, agent):
+        # Return the name of the opponent agent.
+        agent_idx = self.agent_index[agent]  # 1 or 2
+        opponent_idx = 3 - agent_idx  # 1 -> 2, 2 -> 1
+        return self.agents[opponent_idx - 1]
+
+    def reset(self, seed=None, options=None):
+        # Create a new model.
+        self._model = MillModel()
+
+        # Reset the environment variables.
+        self.agents = self.possible_agents[:]
+        self.rewards = {agent: 0 for agent in self.agents}
+        self._cumulative_rewards = {agent: 0 for agent in self.agents}
+        self.terminations = {agent: False for agent in self.agents}
+        self.truncations = {agent: False for agent in self.agents}
+        self.num_moves = 0
+
+        # Compute the legal moves for both players.
+        self.legal_moves = {
+            agent: np.array(self._model.legal_moves(self.agent_index[agent]))
+            for agent in self.agents
+        }
+
+        # Set the info for both players.
+        self.infos = {
+            agent: {
+                'agent': agent,
+                'move': 1,
+                'phase': 'placing',
+                'legal_moves': self.legal_moves[agent]
+            } for agent in self.agents
+        }
+
+        # Set up the first player.
+        self._agent_selector = AgentSelector(self.agents)
+        self.agent_selection = self._agent_selector.next()
+
+        # Render the empty board.
+        self.render()
+
+    def step(self, action):
+        # Get the current player and its opponent.
+        agent = self.agent_selection
+        opponent = self._get_opponent(agent)
+
+        # Calling the step method after the game has finished has no effect.
+        if self._model.game_over():
+            gym.logger.warn(
+                "You are calling step method after the game has finished."
+            )
+            return
+
+        # Actions must be a numpy array.
+        if action is not None:
+            action = np.array(action)
+
+        # Check, if action is legal.
+        legal_moves = self.legal_moves[agent]
+        if action is not None and not np.any(np.all(legal_moves == action, axis=1)):
+            gym.logger.warn(
+                "You are trying to execute an illegal move. A random legal move is chosen instead."
+            )
+            action = None
+
+        # If action is none or not legal, choose a random legal action instead.
+        if action is None:
+            action = legal_moves[np.random.choice(legal_moves.shape[0])]
+
+        # Assert the shape of the action.
+        assert action.shape == (3,)
+
+        # The previous reward has just been observed. Start anew.
+        self._cumulative_rewards[agent] = 0
+
+        # Make the move.
+        move_info = self._model.make_move(self.agent_index[agent], action.tolist())
+
+        # Set the rewards for both players.
+        if move_info['pieces_captured'] > 0:
+            self.rewards[agent] = 1
+            self.rewards[opponent] = -1
+        else:
+            self.rewards[agent] = 0
+            self.rewards[opponent] = 0
+
+        # Add the rewards to the cumulative reward.
+        self._accumulate_rewards()
+
+        # If the last player played, count a move.
+        if self._agent_selector.is_last():
+            self.num_moves += 1
+
+            # Check if the game is too long.
+            self.truncations = {
+                agent: self.num_moves >= 100 for agent in self.agents
+            }
+
+        # Check if the game is over.
+        if self._model.game_over():
+            self.terminations = {
+                agent: True for agent in self.agents
+            }
+
+        # Compute the legal moves for the opponent.
+        self.legal_moves[opponent] = np.array(self._model.legal_moves(self.agent_index[opponent]))
+
+        # Update the agent's info.
+        self.infos[agent]['phase'] = move_info['player_phase']
+        self.infos[opponent]['move'] = self.num_moves + 1
+        self.infos[opponent]['phase'] = move_info['opponent_phase']
+        self.infos[opponent]['legal_moves'] = self.legal_moves[opponent]
+
+        # Set the animation and render.
+        if self.render_mode == 'human':
+            self._animation = {
+                'src': action[0],
+                'dst': action[1],
+                'captured': action[2],
+                'player': agent
+            }
+
+        self.render()
+
+        # Set the next player.
+        self.agent_selection = self._agent_selector.next()
+
+    def render(self):
+        if self.render_mode is None:
+            return
+
+        # Print the ASCII board to the terminal.
+        elif self.render_mode == "ansi":
+            print(self._model)
+
+        # Render the board using pygame.
+        elif self.render_mode == "human":
+            # If we animate the piece, run the animation.
+            if self._animation is not None:
+                # Get player info.
+                player_idx = self.agent_index[self._animation['player']]
+                opponent_idx = self.agent_index[self._get_opponent(self._animation['player'])]
+
+                # Back up the current board.
+                model_backup = self._model.clone()
+
+                # Remove the destination piece. It will be rendered separately.
+                self._model._board[self._animation['dst']] = 0
+
+                # If a piece has been captured, add it back to the board.
+                if self._animation['captured'] > 0:
+                    self._model._board[self._animation['captured']] = opponent_idx
+
+                if self._animation['src'] > 0:
+                    # Set up motion coordinates from src to dst.
+                    (row, col) = self._render_positions[self._animation['src'] - 1]
+                    p0 = (52 + col * 100, 52 + row * 100)
+                    (row, col) = self._render_positions[self._animation['dst'] - 1]
+                    p1 = (52 + col * 100, 52 + row * 100)
+                else:
+                    # Set up motion coordinates from outside the board to dst.
+                    p0 = (352, 754) if player_idx == 1 else (352, -50)
+                    (row, col) = self._render_positions[self._animation['dst'] - 1]
+                    p1 = (52 + col * 100, 52 + row * 100)
+
+                # Animate the move from source to destination.
+                self._animate_board(p0, p1, player_idx)
+
+                # Restore the current board.
+                self._model = model_backup
+
+                # Animate capturing the piece.
+                if self._animation['captured'] > 0:
+                    # Set up motion coordinates.
+                    (row, col) = self._render_positions[self._animation['captured'] - 1]
+                    p0 = (52 + col * 100, 52 + row * 100)
+                    p1 = (352, 754) if player_idx == 1 else (352, -50)
+
+                    # Animate the captured piece flying out.
+                    self._animate_board(p0, p1, opponent_idx)
+
+            # Paint the current board.
+            self._paint_board()
+            self._paint_pieces()
+            self._update_frame()
+
+    def _paint_piece(self, x, y, color1, color2):
+        # Paint a single piece at the given position.
+        surface = self._surface
+        pygame.gfxdraw.filled_circle(surface, x, y, 30, color1)
+        pygame.gfxdraw.aacircle(surface, x, y, 30, color1)
+        pygame.gfxdraw.filled_circle(surface, x, y, 20, color2)
+        pygame.gfxdraw.aacircle(surface, x, y, 20, color2)
+
+    def _paint_pieces(self):
+        # Paint all the pieces on the board.
+        for (i, (row, col)) in enumerate(self._render_positions):
+            position = i + 1
+
+            # Player 1 piece
+            if self._model._board[position] == 1:
+                self._paint_piece(52 + col * 100, 52 + row * 100, (128, 0, 64), (192, 0, 0))
+
+            # Player 2 piece
+            elif self._model._board[position] == 2:
+                self._paint_piece(52 + col * 100, 52 + row * 100, (128, 160, 0), (192, 192, 0))
+
+    def _paint_board(self):
+        surface = self._surface
+
+        # Background
+        surface.fill("tan")
+
+        # Squares
+        pygame.draw.rect(surface, "black", pygame.Rect((47, 47), (610, 610)), 10)
+        pygame.draw.rect(surface, "black", pygame.Rect((147, 147), (410, 410)), 10)
+        pygame.draw.rect(surface, "black", pygame.Rect((247, 247), (210, 210)), 10)
+
+        # Cross
+        pygame.draw.line(surface, "black", pygame.math.Vector2((52, 352)), pygame.math.Vector2((252, 352)), 10)
+        pygame.draw.line(surface, "black", pygame.math.Vector2((452, 352)), pygame.math.Vector2((652, 352)), 10)
+        pygame.draw.line(surface, "black", pygame.math.Vector2((352, 52)), pygame.math.Vector2((352, 252)), 10)
+        pygame.draw.line(surface, "black", pygame.math.Vector2((352, 452)), pygame.math.Vector2((352, 652)), 10)
+
+        # Diagonals
+        pygame.draw.line(surface, "black", pygame.math.Vector2((52, 52)), pygame.math.Vector2((252, 252)), 14)
+        pygame.draw.line(surface, "black", pygame.math.Vector2((452, 452)), pygame.math.Vector2((652, 652)), 14)
+        pygame.draw.line(surface, "black", pygame.math.Vector2((52, 652)), pygame.math.Vector2((252, 452)), 14)
+        pygame.draw.line(surface, "black", pygame.math.Vector2((452, 252)), pygame.math.Vector2((652, 52)), 14)
+
+        # Circles
+        for (i, (row, col)) in enumerate(self._render_positions):
+            position = i + 1
+            pygame.gfxdraw.filled_circle(surface, 52 + col * 100, 52 + row * 100, 10, (0, 0, 0))
+            pygame.gfxdraw.aacircle(surface, 52 + col * 100, 52 + row * 100, 10, (0, 0, 0))
+
+    def _animate_board(self, p0, p1, player):
+        global pygame
+
+        # If episode is truncated, don't animate.
+        if self.truncations[self.agent_selection]:
+            return
+
+        # Decode the starting and ending coordinates.
+        (x0, y0) = p0
+        (x1, y1) = p1
+
+        # Choose the piece color based on the given player index.
+        if player == 1:
+            (color1, color2) = ((128, 0, 64), (192, 0, 0))
+        else:
+            (color1, color2) = ((128, 160, 0), (192, 192, 0))
+
+        # Set the duration in frames.
+        duration = self.metadata['render_fps']
+
+        # Compute the step made in a single frame.
+        dx = float(x1 - x0) / duration
+        dy = float(y1 - y0) / duration
+
+        # Run the animation.
+        x = x0
+        y = y0
+        running = True
+
+        while running:
+            # Check pygame events.
+            for event in pygame.event.get():
+                if event.type == pygame.QUIT:
+                    # Truncate the episode.
+                    self.truncations = {agent: True for agent in self.agents}
+                    running = False
+
+            # Draw the board and the animated piece.
+            self._paint_board()
+            self._paint_pieces()
+            self._paint_piece(round(x), round(y), color1, color2)
+            self._update_frame()
+
+            # Compute the next position of the animated piece.
+            x += dx
+            y += dy
+
+            # Check the length of the animation.
+            duration -= 1
+            if duration == 0:
+                running = False
+
+            # Wait next frame.
+            self._clock.tick(self.metadata['render_fps'])
+
+    def _update_frame(self):
+        if self._frame_callback is not None:
+            self._frame_callback.paint(self._surface)
+        pygame.display.flip()
+
+    def close(self):
+        if self._pygame_initialized:
+            global pygame
+            pygame.quit()
\ No newline at end of file
Index: dqn/train_enhanced.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dqn/train_enhanced.py b/dqn/train_enhanced.py
new file mode 100644
--- /dev/null	(date 1767385615596)
+++ b/dqn/train_enhanced.py	(date 1767385615596)
@@ -0,0 +1,241 @@
+import numpy as np
+import torch
+from famnit_gym.envs import mill
+from dqn.agent import DQNAgent
+from dqn.metrics_tracker import MetricsTracker
+
+
+def calculate_reward(old_state, new_state, player_id):
+    """Calculate sophisticated reward using position evaluation"""
+
+    def evaluate_state(state_model, player):
+        opponent = 3 - player
+        p1_pieces = state_model.count_pieces(player)
+        p2_pieces = state_model.count_pieces(opponent)
+        piece_advantage = (p1_pieces - p2_pieces) * 30
+
+        position_advantage = 0
+        board_state = state_model.get_state()
+
+        position_values = {
+            4: 4, 5: 4, 6: 4, 14: 4, 21: 4, 20: 4, 19: 4, 11: 4,
+            1: 3, 2: 3, 3: 3, 15: 3, 24: 3, 23: 3, 22: 3, 10: 3,
+            7: 3, 8: 3, 9: 3, 13: 3, 18: 3, 17: 3, 16: 3, 12: 3
+        }
+
+        for pos, value in position_values.items():
+            if board_state[pos - 1] == player:
+                position_advantage += value
+            elif board_state[pos - 1] == opponent:
+                position_advantage -= value
+
+        total_score = piece_advantage + position_advantage
+        normalized_score = total_score / 240.0
+        return normalized_score
+
+    old_score = evaluate_state(old_state, player_id)
+    new_score = evaluate_state(new_state, player_id)
+    return new_score - old_score
+
+
+def train_dqn_enhanced(num_episodes=1000, target_update_freq=10, save_freq=100,
+                       log_freq=1, visualization_freq=100):
+    """Train DQN agent with comprehensive tracking"""
+
+    # Create environment
+    env = mill.env(render_mode=None)
+
+    # Create agents
+    agent1 = DQNAgent(player_id=1)
+    agent2 = DQNAgent(player_id=2)
+
+    # Create metrics tracker
+    tracker = MetricsTracker(save_dir='training_logs')
+
+    # Training loop
+    for episode in range(num_episodes):
+        env.reset()
+
+        episode_reward_1 = 0
+        episode_reward_2 = 0
+        episode_losses_1 = []
+        episode_losses_2 = []
+        q_values_1 = []
+        q_values_2 = []
+        step_count = 0
+        outcome = None
+
+        for agent_name in env.agent_iter():
+            observation, env_reward, termination, truncation, info = env.last()
+
+            current_player = 1 if agent_name == "player_1" else 2
+            state = observation
+            agent = agent1 if current_player == 1 else agent2
+
+            # Terminal state
+            if termination or truncation:
+                final_reward = env_reward
+
+                if truncation:
+                    outcome = 'Draw'
+                    final_reward = 0  # No reward for draw
+                elif current_player == 1:
+                    outcome = 'P2 Win'
+                    final_reward = -10  # Lost
+                else:
+                    outcome = 'P1 Win'
+                    final_reward = 10  # Won
+
+                # Store terminal transition
+                if current_player == 1:
+                    episode_reward_1 += final_reward
+                    agent1.store_transition(state, [0, 0, 0], final_reward, state, True)
+                else:
+                    episode_reward_2 += final_reward
+                    agent2.store_transition(state, [0, 0, 0], final_reward, state, True)
+                break
+
+            # Get legal moves and choose action
+            legal_moves = info['legal_moves']
+            action = agent.choose_move(state, legal_moves)
+
+            # Track Q-values
+            with torch.no_grad():
+                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)
+                q_vals = agent.policy_net(state_tensor).cpu().numpy()[0]
+                legal_indices = [agent.action_to_index(move) for move in legal_moves]
+                legal_q_vals = q_vals[legal_indices]
+
+                if current_player == 1:
+                    q_values_1.append({
+                        'mean': np.mean(legal_q_vals),
+                        'max': np.max(legal_q_vals),
+                        'min': np.min(legal_q_vals)
+                    })
+                else:
+                    q_values_2.append({
+                        'mean': np.mean(legal_q_vals),
+                        'max': np.max(legal_q_vals),
+                        'min': np.min(legal_q_vals)
+                    })
+
+            # Store state before move
+            state_before_move = mill.transition_model(env)
+
+            # Take step
+            env.step(action)
+
+            # Get state after move
+            state_after_move = mill.transition_model(env)
+            next_observation, _, _, _, _ = env.last()
+
+            # Calculate shaped reward
+            next_reward = calculate_reward(state_before_move, state_after_move, current_player)
+
+            # Store transition
+            if current_player == 1:
+                episode_reward_1 += next_reward
+                agent1.store_transition(state, action, next_reward, next_observation, False)
+            else:
+                episode_reward_2 += next_reward
+                agent2.store_transition(state, action, next_reward, next_observation, False)
+
+            # Train both agents
+            loss1 = agent1.train()
+            loss2 = agent2.train()
+
+            if loss1 is not None:
+                episode_losses_1.append(loss1)
+            if loss2 is not None:
+                episode_losses_2.append(loss2)
+
+            step_count += 1
+
+        # Decay epsilon
+        agent1.decay_epsilon()
+        agent2.decay_epsilon()
+
+        # Update target networks
+        if episode % target_update_freq == 0:
+            agent1.update_target_network()
+            agent2.update_target_network()
+
+        # Log metrics
+        if episode % log_freq == 0:
+            tracker.log_episode(
+                episode=episode,
+                reward_p1=episode_reward_1,
+                reward_p2=episode_reward_2,
+                loss_p1=np.mean(episode_losses_1) if episode_losses_1 else None,
+                loss_p2=np.mean(episode_losses_2) if episode_losses_2 else None,
+                epsilon_p1=agent1.epsilon,
+                epsilon_p2=agent2.epsilon,
+                episode_length=step_count,
+                outcome=outcome,
+                mean_q_value_p1=np.mean([q['mean'] for q in q_values_1]) if q_values_1 else 0,
+                max_q_value_p1=np.max([q['max'] for q in q_values_1]) if q_values_1 else 0,
+                mean_q_value_p2=np.mean([q['mean'] for q in q_values_2]) if q_values_2 else 0,
+                max_q_value_p2=np.max([q['max'] for q in q_values_2]) if q_values_2 else 0,
+                buffer_size_p1=len(agent1.memory),
+                buffer_size_p2=len(agent2.memory)
+            )
+
+        # Console logging
+        if episode % 10 == 0:
+            recent_rewards_1 = [tracker.metrics['reward_p1'][i]
+                                for i in range(max(0, len(tracker.metrics['reward_p1']) - 10),
+                                               len(tracker.metrics['reward_p1']))]
+            recent_rewards_2 = [tracker.metrics['reward_p2'][i]
+                                for i in range(max(0, len(tracker.metrics['reward_p2']) - 10),
+                                               len(tracker.metrics['reward_p2']))]
+
+            print(f"\n{'=' * 60}")
+            print(f"Episode {episode}/{num_episodes}")
+            print(f"{'=' * 60}")
+            print(f"Avg Reward (last 10):  P1: {np.mean(recent_rewards_1):.3f}  |  P2: {np.mean(recent_rewards_2):.3f}")
+            print(f"Epsilon:               P1: {agent1.epsilon:.3f}  |  P2: {agent2.epsilon:.3f}")
+            print(f"Buffer Size:           P1: {len(agent1.memory):5d}  |  P2: {len(agent2.memory):5d}")
+            print(f"Episode Length:        {step_count} steps")
+            print(f"Outcome:               {outcome}")
+            if episode_losses_1:
+                print(
+                    f"Avg Loss:              P1: {np.mean(episode_losses_1):.4f}  |  P2: {np.mean(episode_losses_2):.4f}")
+
+        # Save models
+        if episode % save_freq == 0 and episode > 0:
+            agent1.save(f'dqn_models/agent1_episode_{episode}.pth')
+            agent2.save(f'dqn_models/agent2_episode_{episode}.pth')
+            print(f"\nâ Models saved at episode {episode}")
+
+        # Generate visualizations
+        if episode % visualization_freq == 0 and episode > 0:
+            tracker.plot_training_curves(f'training_curves_ep{episode}.png')
+            tracker.save_metrics(f'metrics_ep{episode}.json')
+            print(f"â Visualizations generated at episode {episode}")
+
+    # Final save
+    agent1.save('dqn_models/agent1_final.pth')
+    agent2.save('dqn_models/agent2_final.pth')
+    tracker.plot_training_curves('training_curves_final.png')
+    tracker.save_metrics('metrics_final.json')
+
+    print("\n" + "=" * 60)
+    print("Training Complete!")
+    print("=" * 60)
+
+    return agent1, agent2, tracker
+
+
+if __name__ == "__main__":
+    import os
+
+    os.makedirs('dqn_models', exist_ok=True)
+    os.makedirs('training_logs', exist_ok=True)
+
+    train_dqn_enhanced(
+        num_episodes=1000,
+        target_update_freq=10,
+        save_freq=100,
+        log_freq=1,
+        visualization_freq=100
+    )
\ No newline at end of file
Index: dqn/metrics_tracker.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dqn/metrics_tracker.py b/dqn/metrics_tracker.py
new file mode 100644
--- /dev/null	(date 1767385586353)
+++ b/dqn/metrics_tracker.py	(date 1767385586353)
@@ -0,0 +1,189 @@
+import json
+import numpy as np
+from collections import defaultdict
+import matplotlib.pyplot as plt
+import seaborn as sns
+from pathlib import Path
+
+
+class MetricsTracker:
+    def __init__(self, save_dir='training_logs'):
+        self.save_dir = Path(save_dir)
+        self.save_dir.mkdir(exist_ok=True)
+
+        # Training metrics
+        self.metrics = defaultdict(list)
+
+    def log_episode(self, episode, **kwargs):
+        """Log metrics for an episode"""
+        self.metrics['episode'].append(episode)
+        for key, value in kwargs.items():
+            self.metrics[key].append(value)
+
+    def save_metrics(self, filename='metrics.json'):
+        """Save all metrics to JSON"""
+        filepath = self.save_dir / filename
+        with open(filepath, 'w') as f:
+            json.dump(dict(self.metrics), f, indent=2)
+
+    def get_moving_average(self, key, window=100):
+        """Calculate moving average for a metric"""
+        values = self.metrics[key]
+        if len(values) < window:
+            return values
+        return np.convolve(values, np.ones(window) / window, mode='valid')
+
+    def plot_training_curves(self, save_path='training_curves.png'):
+        """Generate comprehensive training visualization"""
+        fig, axes = plt.subplots(3, 3, figsize=(18, 15))
+        fig.suptitle('DQN Training Metrics', fontsize=16, fontweight='bold')
+
+        episodes = self.metrics['episode']
+
+        # 1. Rewards
+        ax = axes[0, 0]
+        if 'reward_p1' in self.metrics:
+            ax.plot(episodes, self.metrics['reward_p1'], alpha=0.3, label='P1 Raw')
+            ax.plot(episodes, self.get_moving_average('reward_p1'), label='P1 MA(100)', linewidth=2)
+        if 'reward_p2' in self.metrics:
+            ax.plot(episodes, self.metrics['reward_p2'], alpha=0.3, label='P2 Raw')
+            ax.plot(episodes, self.get_moving_average('reward_p2'), label='P2 MA(100)', linewidth=2)
+        ax.set_xlabel('Episode')
+        ax.set_ylabel('Total Reward')
+        ax.set_title('Episode Rewards')
+        ax.legend()
+        ax.grid(True, alpha=0.3)
+
+        # 2. Loss
+        ax = axes[0, 1]
+        if 'loss_p1' in self.metrics:
+            loss_p1_filtered = [x for x in self.metrics['loss_p1'] if x is not None]
+            ax.plot(loss_p1_filtered, alpha=0.3, label='P1 Raw')
+            if len(loss_p1_filtered) > 100:
+                ax.plot(self.get_moving_average('loss_p1', window=100), label='P1 MA(100)', linewidth=2)
+        if 'loss_p2' in self.metrics:
+            loss_p2_filtered = [x for x in self.metrics['loss_p2'] if x is not None]
+            ax.plot(loss_p2_filtered, alpha=0.3, label='P2 Raw')
+            if len(loss_p2_filtered) > 100:
+                ax.plot(self.get_moving_average('loss_p2', window=100), label='P2 MA(100)', linewidth=2)
+        ax.set_xlabel('Training Step')
+        ax.set_ylabel('Loss')
+        ax.set_title('Training Loss')
+        ax.legend()
+        ax.grid(True, alpha=0.3)
+        ax.set_yscale('log')
+
+        # 3. Epsilon Decay
+        ax = axes[0, 2]
+        if 'epsilon_p1' in self.metrics:
+            ax.plot(episodes, self.metrics['epsilon_p1'], label='P1')
+        if 'epsilon_p2' in self.metrics:
+            ax.plot(episodes, self.metrics['epsilon_p2'], label='P2')
+        ax.set_xlabel('Episode')
+        ax.set_ylabel('Epsilon')
+        ax.set_title('Exploration Rate (Îµ)')
+        ax.legend()
+        ax.grid(True, alpha=0.3)
+
+        # 4. Episode Length
+        ax = axes[1, 0]
+        if 'episode_length' in self.metrics:
+            ax.plot(episodes, self.metrics['episode_length'], alpha=0.3, label='Raw')
+            ax.plot(episodes, self.get_moving_average('episode_length'), label='MA(100)', linewidth=2)
+        ax.set_xlabel('Episode')
+        ax.set_ylabel('Steps')
+        ax.set_title('Episode Length')
+        ax.legend()
+        ax.grid(True, alpha=0.3)
+
+        # 5. Win/Loss/Draw Distribution (Last 100 episodes)
+        ax = axes[1, 1]
+        if 'outcome' in self.metrics:
+            last_100 = self.metrics['outcome'][-100:]
+            outcomes = {'P1 Win': 0, 'P2 Win': 0, 'Draw': 0}
+            for outcome in last_100:
+                outcomes[outcome] += 1
+            ax.bar(outcomes.keys(), outcomes.values(), color=['#2ecc71', '#e74c3c', '#95a5a6'])
+            ax.set_ylabel('Count')
+            ax.set_title('Outcomes (Last 100 Episodes)')
+            ax.grid(True, alpha=0.3, axis='y')
+
+        # 6. Cumulative Win Rate
+        ax = axes[1, 2]
+        if 'outcome' in self.metrics:
+            p1_wins = []
+            p2_wins = []
+            draws = []
+            p1_count = 0
+            p2_count = 0
+            draw_count = 0
+            total = 0
+
+            for outcome in self.metrics['outcome']:
+                total += 1
+                if outcome == 'P1 Win':
+                    p1_count += 1
+                elif outcome == 'P2 Win':
+                    p2_count += 1
+                else:
+                    draw_count += 1
+
+                p1_wins.append(p1_count / total * 100)
+                p2_wins.append(p2_count / total * 100)
+                draws.append(draw_count / total * 100)
+
+            ax.plot(episodes, p1_wins, label='P1 Win Rate', linewidth=2)
+            ax.plot(episodes, p2_wins, label='P2 Win Rate', linewidth=2)
+            ax.plot(episodes, draws, label='Draw Rate', linewidth=2)
+            ax.set_xlabel('Episode')
+            ax.set_ylabel('Win Rate (%)')
+            ax.set_title('Cumulative Win Rates')
+            ax.legend()
+            ax.grid(True, alpha=0.3)
+
+        # 7. Q-Values Statistics
+        ax = axes[2, 0]
+        if 'mean_q_value_p1' in self.metrics:
+            ax.plot(episodes, self.metrics['mean_q_value_p1'], label='P1 Mean', alpha=0.7)
+            if 'max_q_value_p1' in self.metrics:
+                ax.fill_between(episodes,
+                                self.metrics['mean_q_value_p1'],
+                                self.metrics['max_q_value_p1'],
+                                alpha=0.2, label='P1 Range')
+        if 'mean_q_value_p2' in self.metrics:
+            ax.plot(episodes, self.metrics['mean_q_value_p2'], label='P2 Mean', alpha=0.7)
+        ax.set_xlabel('Episode')
+        ax.set_ylabel('Q-Value')
+        ax.set_title('Q-Value Statistics')
+        ax.legend()
+        ax.grid(True, alpha=0.3)
+
+        # 8. Reward Distribution (Histogram)
+        ax = axes[2, 1]
+        if 'reward_p1' in self.metrics and len(self.metrics['reward_p1']) > 0:
+            ax.hist(self.metrics['reward_p1'], bins=50, alpha=0.5, label='P1', color='blue')
+        if 'reward_p2' in self.metrics and len(self.metrics['reward_p2']) > 0:
+            ax.hist(self.metrics['reward_p2'], bins=50, alpha=0.5, label='P2', color='orange')
+        ax.set_xlabel('Total Episode Reward')
+        ax.set_ylabel('Frequency')
+        ax.set_title('Reward Distribution')
+        ax.legend()
+        ax.grid(True, alpha=0.3, axis='y')
+
+        # 9. Buffer Usage
+        ax = axes[2, 2]
+        if 'buffer_size_p1' in self.metrics:
+            ax.plot(episodes, self.metrics['buffer_size_p1'], label='P1')
+        if 'buffer_size_p2' in self.metrics:
+            ax.plot(episodes, self.metrics['buffer_size_p2'], label='P2')
+        ax.set_xlabel('Episode')
+        ax.set_ylabel('Buffer Size')
+        ax.set_title('Replay Buffer Usage')
+        ax.legend()
+        ax.grid(True, alpha=0.3)
+
+        plt.tight_layout()
+        plt.savefig(self.save_dir / save_path, dpi=150, bbox_inches='tight')
+        plt.close()
+
+        print(f"Training curves saved to {self.save_dir / save_path}")
\ No newline at end of file
Index: dqn/compare_with_minimax.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dqn/compare_with_minimax.py b/dqn/compare_with_minimax.py
new file mode 100644
--- /dev/null	(date 1767385685590)
+++ b/dqn/compare_with_minimax.py	(date 1767385685590)
@@ -0,0 +1,86 @@
+import numpy as np
+import matplotlib.pyplot as plt
+from famnit_gym.envs import mill
+from dqn.agent import DQNAgent
+from minimax_implementations.limited_depth import find_optimal_move as minimax_move
+
+
+def test_dqn_vs_minimax(dqn_model_path, minimax_depth=3, num_games=100):
+    """Test DQN agent against minimax baseline"""
+
+    env = mill.env(render_mode=None)
+    dqn_agent = DQNAgent(player_id=1)
+    dqn_agent.load(dqn_model_path)
+    dqn_agent.epsilon = 0.0
+
+    results = {'DQN Wins': 0, 'Minimax Wins': 0, 'Draws': 0}
+    episode_lengths = []
+
+    for game in range(num_games):
+        env.reset()
+        steps = 0
+
+        for agent_name in env.agent_iter():
+            observation, reward, termination, truncation, info = env.last()
+
+            current_player = 1 if agent_name == "player_1" else 2
+            legal_moves = info['legal_moves']
+
+            if termination or truncation:
+                if truncation:
+                    results['Draws'] += 1
+                elif current_player == 1:
+                    results['Minimax Wins'] += 1
+                else:
+                    results['DQN Wins'] += 1
+                episode_lengths.append(steps)
+                break
+
+            # DQN agent (Player 1)
+            if current_player == 1:
+                action = dqn_agent.choose_move(observation, legal_moves)
+            # Minimax (Player 2)
+            else:
+                state = mill.transition_model(env.unwrapped)
+                action = minimax_move(state, current_player, minimax_depth, steps)
+
+            env.step(action)
+            steps += 1
+
+        if (game + 1) % 10 == 0:
+            print(f"Games {game + 1}/{num_games}: {results}")
+
+    # Visualization
+    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
+
+    # Win rates
+    ax = axes[0]
+    ax.bar(results.keys(), results.values(), color=['#2ecc71', '#e74c3c', '#95a5a6'])
+    ax.set_ylabel('Games')
+    ax.set_title(f'DQN vs Minimax (Depth {minimax_depth}) - {num_games} Games')
+    ax.grid(True, alpha=0.3, axis='y')
+
+    # Episode lengths
+    ax = axes[1]
+    ax.hist(episode_lengths, bins=20, color='steelblue', alpha=0.7, edgecolor='black')
+    ax.axvline(np.mean(episode_lengths), color='red', linestyle='--',
+               linewidth=2, label=f'Mean: {np.mean(episode_lengths):.1f}')
+    ax.set_xlabel('Episode Length (steps)')
+    ax.set_ylabel('Frequency')
+    ax.set_title('Game Length Distribution')
+    ax.legend()
+    ax.grid(True, alpha=0.3, axis='y')
+
+    plt.tight_layout()
+    plt.savefig('training_logs/dqn_vs_minimax.png', dpi=150)
+    plt.close()
+
+    print(f"\nFinal Results:")
+    print(f"  DQN Wins: {results['DQN Wins']} ({results['DQN Wins'] / num_games * 100:.1f}%)")
+    print(f"  Minimax Wins: {results['Minimax Wins']} ({results['Minimax Wins'] / num_games * 100:.1f}%)")
+    print(f"  Draws: {results['Draws']} ({results['Draws'] / num_games * 100:.1f}%)")
+    print(f"  Avg Episode Length: {np.mean(episode_lengths):.1f} steps")
+
+
+if __name__ == "__main__":
+    test_dqn_vs_minimax('dqn_models/agent1_final.pth', minimax_depth=3, num_games=100)
\ No newline at end of file
Index: dqn/visualize_policy.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dqn/visualize_policy.py b/dqn/visualize_policy.py
new file mode 100644
--- /dev/null	(date 1767385647337)
+++ b/dqn/visualize_policy.py	(date 1767385647337)
@@ -0,0 +1,97 @@
+import numpy as np
+import matplotlib.pyplot as plt
+import seaborn as sns
+import torch
+from famnit_gym.envs import mill
+from dqn.agent import DQNAgent
+
+
+def visualize_action_preferences(model_path, num_samples=100):
+    """Visualize which board positions the agent prefers to place/move pieces"""
+
+    agent = DQNAgent(player_id=1)
+    agent.load(model_path)
+    agent.epsilon = 0.0
+
+    # Track action statistics
+    placement_heatmap = np.zeros(24)  # Destination positions for placing
+    move_from_heatmap = np.zeros(24)  # Source positions for moving
+    move_to_heatmap = np.zeros(24)  # Destination positions for moving
+    capture_heatmap = np.zeros(24)  # Captured positions
+
+    env = mill.env(render_mode=None)
+
+    for _ in range(num_samples):
+        env.reset()
+
+        for agent_name in env.agent_iter():
+            observation, reward, termination, truncation, info = env.last()
+
+            if termination or truncation:
+                break
+
+            current_player = 1 if agent_name == "player_1" else 2
+
+            if current_player == 1:  # Only track agent's moves
+                legal_moves = info['legal_moves']
+                action = agent.choose_move(observation, legal_moves)
+
+                # Track statistics
+                if action[0] == 0:  # Placing phase
+                    placement_heatmap[action[1] - 1] += 1
+                else:  # Moving/Flying phase
+                    move_from_heatmap[action[0] - 1] += 1
+                    move_to_heatmap[action[1] - 1] += 1
+
+                if action[2] > 0:  # Capture
+                    capture_heatmap[action[2] - 1] += 1
+
+                env.step(action)
+            else:
+                # Random opponent
+                legal_moves = info['legal_moves']
+                action = legal_moves[np.random.choice(len(legal_moves))]
+                env.step(action)
+
+    # Create visualization
+    fig, axes = plt.subplots(2, 2, figsize=(14, 12))
+
+    # Mill board positions (visualize as 3x3 grids)
+    def plot_heatmap(ax, data, title):
+        # Reshape to approximate board layout
+        # This is a simplified visualization
+        board_layout = np.array([
+            [data[0], data[1], data[2]],
+            [data[9], data[10], data[14]],
+            [data[21], data[22], data[23]]
+        ])
+
+        sns.heatmap(board_layout, annot=True, fmt='.0f', cmap='YlOrRd',
+                    ax=ax, cbar_kws={'label': 'Frequency'})
+        ax.set_title(title, fontsize=12, fontweight='bold')
+        ax.set_xlabel('')
+        ax.set_ylabel('')
+
+    # Bar chart for all positions
+    def plot_bar(ax, data, title):
+        positions = np.arange(1, 25)
+        ax.bar(positions, data, color='steelblue', alpha=0.7)
+        ax.set_xlabel('Board Position')
+        ax.set_ylabel('Frequency')
+        ax.set_title(title, fontsize=12, fontweight='bold')
+        ax.grid(True, alpha=0.3, axis='y')
+
+    plot_bar(axes[0, 0], placement_heatmap, 'Placement Preferences')
+    plot_bar(axes[0, 1], move_from_heatmap, 'Move Source Preferences')
+    plot_bar(axes[1, 0], move_to_heatmap, 'Move Destination Preferences')
+    plot_bar(axes[1, 1], capture_heatmap, 'Capture Position Preferences')
+
+    plt.tight_layout()
+    plt.savefig('training_logs/action_preferences.png', dpi=150, bbox_inches='tight')
+    plt.close()
+
+    print("Action preference visualization saved to training_logs/action_preferences.png")
+
+
+if __name__ == "__main__":
+    visualize_action_preferences('dqn_models/agent1_final.pth', num_samples=100)
\ No newline at end of file
